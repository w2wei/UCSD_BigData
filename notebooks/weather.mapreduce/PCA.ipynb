{
 "metadata": {
  "name": "",
  "signature": "sha256:8350d73e416f7a6aadba6269980bd6cda3a093d835be56dcad6aa218065763fb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "import sys\n",
      "home_dir='/home/ubuntu/UCSD_BigData'\n",
      "dat_dir='/home/ubuntu/UCSD_BigData/data/weather'\n",
      "sys.path.append(home_dir+'/utils')\n",
      "from find_waiting_flow import *\n",
      "from AWS_keypair_management import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "localData=home_dir+'/data/weather/ALL.head.csv'\n",
      "#!cat $localData"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "station=dat_dir+'/ghcnd-stations.txt'\n",
      "#!less $station"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "readme = dat_dir+'/ghcnd-readme.txt'\n",
      "#!cat $readme"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile element_dist.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "summarize the distribution of measurements\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "from sys import stderr\n",
      "\n",
      "class measDist(MRJob):\n",
      "\n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            elements=line.split(',')\n",
      "            if elements[0]=='station':\n",
      "                out=('header',1)\n",
      "            else:\n",
      "                out=(elements[1],1)\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+line)\n",
      "            stderr.write(e)\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "            out=('error',1)\n",
      "\n",
      "        finally:\n",
      "            yield out\n",
      "            \n",
      "    def combiner(self, word, counts):\n",
      "        self.increment_counter('MrJob Counters','combiner',1)\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "    def reducer(self, word, counts):\n",
      "        self.increment_counter('MrJob Counters','reducer',1)\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    measDist.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting element_dist.py\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "Creds= pickle.load(open('/home/ubuntu/Vault/Creds.pkl','rb'))\n",
      "pair=Creds['mrjob']\n",
      "key_id=pair['key_id']\n",
      "secret_key=pair['secret_key']\n",
      "ID=pair['ID']\n",
      "job_flow_id=find_waiting_flow(key_id,secret_key)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!python element_dist.py -r emr --emr-job-flow-id  $job_flow_id /home/ubuntu/UCSD_BigData/data/weather/ALL.head.csv > counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile temp_dist.py\n",
      "## summarize all TMAX and TMIN measurements across stations\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "summarize the distribution of tmax and tmin\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "from sys import stderr\n",
      "\n",
      "class temp(MRJob):\n",
      "\n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            elements=line.split(',')\n",
      "            if elements[1]=='TMAX':\n",
      "                out=('tmax',1)\n",
      "            elif elements[1]=='TMIN':\n",
      "                out=('tmin',1)\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+line)\n",
      "            stderr.write(e)\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "            out=('error',1)\n",
      "\n",
      "        finally:\n",
      "            yield out\n",
      "            \n",
      "    def combiner(self, word, counts):\n",
      "        self.increment_counter('MrJob Counters','combiner',1)\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "    def reducer(self, word, counts):\n",
      "        self.increment_counter('MrJob Counters','reducer',1)\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    temp.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting temp_dist.py\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!python element_dist.py -r local /home/ubuntu/UCSD_BigData/data/weather/ALL.head.csv > temp_dist.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## copy Weather.GHNC from professor's bucket\n",
      "from boto.s3.connection import S3Connection,OrdinaryCallingFormat\n",
      "from boto.s3.key import Key\n",
      "conn = S3Connection(key_id, secret_key,calling_format = OrdinaryCallingFormat())\n",
      "# weather = conn.get_bucket(\"Weather.GHNC\")\n",
      "# # download a S3 file to local disk\n",
      "# key = weather.get_key('compiled_statistics.pkl.gz')\n",
      "# key.get_contents_to_filename('/home/ubuntu/UCSD_BigData/data/weather/compiled_statistics.pkl.gz')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile sample.py\n",
      "# sample hdfs:/weather/weather.csv to make a test set.\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "sample ration 1/1000\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "from sys import stderr\n",
      "import random\n",
      "\n",
      "\n",
      "class sample(MRJob):\n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            if random.random()<0.001:\n",
      "                yield (None,line)\n",
      "        except Exception, e:\n",
      "            stderr.write(str(e))\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    sample.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting sample.py\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# clean the data and create a subset\n",
      "subdir=\"/home/ubuntu/UCSD_BigData/data/weather/\"\n",
      "dat = file(subdir+\"subset\").readlines()\n",
      "fout=file(subdir+\"subset_9k\",\"wb\")\n",
      "for d in dat:\n",
      "    fout.write(d.split(\"\\t\")[1].split(\"\\n\")[0].strip('\\\"')+\"\\n\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subdir=\"/home/ubuntu/UCSD_BigData/data/weather/\"\n",
      "subset9k= subdir+\"subset_9k\"\n",
      "#!less $subset9k"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile meas_count.py\n",
      "## count number of valid station/year. >50% TMAX and TMIN \n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "count the number of valid measurements. A valide record has at least 50% days with TMAX and TMIN.\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "from sys import stderr\n",
      "\n",
      "class countMeas(MRJob):\n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            rec = line.split(\",\")\n",
      "            stn,meas,year = rec[:3]\n",
      "            stderr.write(stn+\"\\t\"+meas+\"\\t\"+year)\n",
      "            if meas=='TMAX' or meas=='TMIN': # this is measurement is about tmax of tmin\n",
      "                if len(filter(None,rec[3:]))/365.0>0.5: # measurements during >50% days of the year \n",
      "                    stderr.write(\"mapper: \"+\",\".join(rec[:3]))\n",
      "                    yield (stn,1)\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+str(line[:5]))\n",
      "            stderr.write(str(e))\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "\n",
      "    def reducer(self, key, value):\n",
      "        self.increment_counter('MrJob Counters','reducer',1)\n",
      "        try:\n",
      "            l_counts=[v for v in value]\n",
      "            s=sum(l_counts)\n",
      "            yield (key,s)\n",
      "\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    countMeas.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting meas_count.py\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# !python meas_count.py -r local /home/ubuntu/UCSD_BigData/data/weather/subset_9k  > meas_weight\n",
      "#!python meas_count.py -r emr --emr-job-flow-id  $job_flow_id hdfs:/weather/weather.csv  > meas_weight_all # "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!python meas_count.py -r emr --emr-job-flow-id  $job_flow_id hdfs:/weather/weather.csv  > meas_weight_all # "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import re\n",
      "import pandas as pd\n",
      "import pickle\n",
      "# create or load a dataframe of station meta data\n",
      "try:\n",
      "    stationDF=pickle.load(file(os.path.join(dat_dir,\"stationDF.pkl\")))\n",
      "except:\n",
      "    stnDat = file(os.path.join(dat_dir,\"ghcnd-stations.txt\"),'rb').readlines()\n",
      "    stn = [re.split(\" +\", st)[:4] for st in stnDat]\n",
      "    ind = [l[0] for l in stn] # get the index\n",
      "    stn = [[st[0],float(st[1]),float(st[2]),float(st[3])] for st in stn]\n",
      "    stnDF = pd.DataFrame(data=stn,index=ind,columns=['ID','latitude','longitude','elevation'])\n",
      "    \n",
      "#     pickle.dump(stnDF,file(os.path.join(dat_dir,\"stationDF.pkl\"),\"wb\"))\n",
      "    # append a column of measurement weights to this metadata dataframe\n",
      "    measWT = file(\"meas_weight_all\").readlines()\n",
      "    measWT=[filter(None,re.split(\"[\\t\\n\\\"]\",rec)) for rec in measWT]\n",
      "    measWT=[[x[0],int(x[1])] for x in measWT]\n",
      "    ind = [m[0] for m in measWT]\n",
      "    measDF = pd.DataFrame(data=measWT,index=ind,columns=[\"ID\",\"weight\"])\n",
      "    del measDF[\"ID\"]\n",
      "    stationDF = stnDF.merge(measDF,how='outer',left_index=True,right_index=True)\n",
      "    pickle.dump(stationDF,file(os.path.join(dat_dir,\"stationDF.pkl\"),\"wb\"))\n",
      "    \n",
      "finally:\n",
      "    print stationDF.ix[:10,]\n",
      "    testDF = stationDF[stationDF['weight']>0] # 27302 non NaN records"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                      ID  latitude  longitude  elevation  weight\n",
        "ACW00011604  ACW00011604   17.1167   -61.7833       10.1       2\n",
        "ACW00011647  ACW00011647   17.1333   -61.7833       19.2     NaN\n",
        "AE000041196  AE000041196   25.3330    55.5170       34.0      83\n",
        "AF000040930  AF000040930   35.3170    69.0170     3366.0       3\n",
        "AG000060390  AG000060390   36.7167     3.2500       24.0     146\n",
        "AG000060590  AG000060590   30.5667     2.8667      397.0     146\n",
        "AG000060611  AG000060611   28.0500     9.6331      561.0     108\n",
        "AG000060680  AG000060680   22.8000     5.4331     1362.0     146\n",
        "AGE00135039  AGE00135039   35.7297     0.6500       50.0     126\n",
        "AJ000037575  AJ000037575   41.5500    46.6670      490.0      71\n",
        "\n",
        "[10 rows x 5 columns]\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = testDF[:5]\n",
      "test['weight']=range(1,6)\n",
      "sortedTest = test.sort(columns=['latitude'])\n",
      "import numpy as np\n",
      "\n",
      "def cdf(df):\n",
      "    df['cdf']= np.zeros(len((df['weight'])))\n",
      "    df['cdf'][0]=df['weight'][0]\n",
      "    for i in xrange(1,len(df['weight'])):\n",
      "        df['cdf'][i]=df['cdf'][i-1]+df['weight'][i]\n",
      "    return df['cdf']\n",
      "\n",
      "# # cdf.median = sortedTest['cdf'].median()\n",
      "\n",
      "def buildTree(df,latflg=1,it=1):\n",
      "#     print \"Iteration \",it\n",
      "    it+=1\n",
      "    if df.shape[0]<2:\n",
      "#         print \"leaf: \\n\",df\n",
      "        return df\n",
      "    else:\n",
      "        if latflg==1:\n",
      "            sortedDF = df.sort(columns=['latitude'])\n",
      "        else:\n",
      "            sortedDF = df.sort(columns=['longitude'])\n",
      "        latflg=-latflg # switch sorting direction\n",
      "        sortedDF['cdf']=cdf(sortedDF)#pd.Series(cdf(sortedDF['weight']),index=sortedDF.index)\n",
      "        mid = sortedDF['cdf'][-1]/2.0\n",
      "        sortedDFL = sortedDF[sortedDF['cdf']<=mid]\n",
      "        sortedDFR = sortedDF[sortedDF['cdf']>mid]\n",
      "#         print \"L \\n\",sortedDFL\n",
      "#         print \"R \\n\",sortedDFR\n",
      "        buildTree(sortedDFL,latflg,it)\n",
      "        buildTree(sortedDFR,latflg,it)\n",
      "buildTree(test)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "PCA starts here, using Professor's pre-built partitioned tree and stations."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "tree = pickle.load(open('/home/ubuntu/UCSD_BigData/data/weather/tree.pkl'))\n",
      "ptree = tree['Partition_Tree']\n",
      "pstn = tree['Partitioned_Stations']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ptree[ptree['level']==5]\n",
      "print ptree.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "       coord  counter         key  threshold  level\n",
        "0   latitude      875   101100011    38.2875      9\n",
        "1  longitude     1146  0001110001  -111.8997     10\n",
        "2   latitude      603   001010011    31.1333      9\n",
        "3  longitude      516    11111101    66.0500      8\n",
        "4  longitude       78      001001  -100.2831      6\n",
        "\n",
        "[5 rows x 5 columns]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile concatTminTmax.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "Given a station ID, yield all availble data using mapreduce\n",
      "take USC00515000 as an example\n",
      "\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "from sys import stderr\n",
      "import pandas as pd\n",
      "import re,pickle,base64,zlib\n",
      "\n",
      "def decode(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n",
      "\n",
      "def encode(Value):\n",
      "    \"\"\" Encode a value as a string \"\"\"\n",
      "    return base64.b64encode(zlib.compress(pickle.dumps(Value),9))\n",
      " \n",
      "\n",
      "class concatTminTmax(MRJob):\n",
      "    def concat_mapper(self, _, line):\n",
      "        try:\n",
      "            rec = line.split(\",\")\n",
      "            stn,meas,year = rec[:3]\n",
      "            if meas=='TMAX' or meas=='TMIN': # this is measurement is about tmax of tmin\n",
      "                if len(filter(None,rec[3:]))/365.0>0.5: # measurements during >50% days of the year \n",
      "                    yield stn+\":\"+year,encode([meas,rec[3:]])\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+str(line[:5]))\n",
      "            stderr.write(str(e))\n",
      "\n",
      "    def concat_reducer(self, key, value):\n",
      "        try:\n",
      "            stn,year=key.split(\":\")\n",
      "            recs = list(value)\n",
      "            trec={}\n",
      "            if len(recs)==2:\n",
      "                stderr.write(stn+\"\\t\"+year+\"\\n\")\n",
      "                for r in recs:\n",
      "                    meas,vec = decode(r)\n",
      "                    trec[meas]=vec\n",
      "                #k1,k2=trec.keys() # for test \n",
      "                #stderr.write(stn+\"\\t\"+year+\"\\t\"+k1+\"\\t\"+str(trec[k1][:10])+\"\\t\"+k2+\"\\t\"+str(trec[k2][:10])+\"\\n\")\n",
      "                yield stn,encode(trec)\n",
      "\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e)+\"\\t\")\n",
      "    \n",
      "\n",
      "    def steps(self):\n",
      "        return [\n",
      "            self.mr(mapper=self.concat_mapper,\n",
      "                    reducer=self.concat_reducer)\n",
      "        ]\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    concatTminTmax.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting concatTminTmax.py\n"
       ]
      }
     ],
     "prompt_number": 261
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python concatTminTmax.py -r emr --emr-job-flow-id  $job_flow_id hdfs:/weather/weather.csv --output-dir s3://weiwei.bucket/data/all_dat.csv/  --no-output\n",
      "#!python meas_count.py -r emr --emr-job-flow-id  $job_flow_id hdfs:/weather/weather.csv  > meas_weight_all # "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /tmp/concatTminTmax.ubuntu.20140526.053832.184200\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://weiwei.bucket/scratch/concatTminTmax.ubuntu.20140526.053832.184200/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow j-LTOJMJ14G840\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 31.0s ago, status RUNNING: Running step (concatTminTmax.ubuntu.20140526.053832.184200: Step 1 of 1)\r\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile samplePCADat.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "create a sample set for PCA test\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "from sys import stderr\n",
      "import random\n",
      "import re,pickle,base64,zlib\n",
      "\n",
      "def decode(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n",
      "\n",
      "def encode(Value):\n",
      "    \"\"\" Encode a value as a string \"\"\"\n",
      "    return base64.b64encode(zlib.compress(pickle.dumps(Value),9))\n",
      "\n",
      "class samplePCADat(MRJob):\n",
      "    def sample_dat_mapper(self,_,line):\n",
      "        try:\n",
      "            stn,trecEncode = line.split(\"\\t\")\n",
      "            stderr.write(\"Line: \"+\"\\t\"+stn+\"\\t\"+trecEncode[:10]+\"\\n\")\n",
      "            if random.random()<0.01:\n",
      "                yield stn.strip('\\\"'),trecEncode.strip('\\\"')\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+str(line[:5]))\n",
      "            stderr.write(str(e))\n",
      "\n",
      "    def steps(self):\n",
      "        return [\n",
      "            self.mr(mapper=self.sample_dat_mapper)\n",
      "        ]\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    samplePCADat.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting samplePCADat.py\n"
       ]
      }
     ],
     "prompt_number": 372
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python samplePCADat.py -r emr --emr-job-flow-id  $job_flow_id s3://weiwei.bucket/data/all_dat.csv/ > sample_dat_10k.csv \n",
      "#--output-dir s3://weiwei.bucket/data/sample_dat.csv/  --no-output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /tmp/samplePCADat.ubuntu.20140526.192324.390970\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://weiwei.bucket/scratch/samplePCADat.ubuntu.20140526.192324.390970/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow j-LTOJMJ14G840\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 31.0s ago, status RUNNING: Running step (samplePCADat.ubuntu.20140526.192324.390970: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 62.1s ago, status RUNNING: Running step (samplePCADat.ubuntu.20140526.192324.390970: Step 1 of 1)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job completed.\r\n",
        "Running time was 55.0s (not counting time spent waiting for the EC2 instances)\r\n",
        "ec2_key_pair_file not specified, going to S3\r\n",
        "Fetching counters from S3...\r\n",
        "Waiting 5.0s for S3 eventual consistency\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters may not have been uploaded to S3 yet. Try again in 5 minutes with: mrjob fetch-logs --counters j-LTOJMJ14G840\r\n",
        "Counters from step 1:\r\n",
        "  (no counters found)\r\n",
        "Streaming final output from s3://weiwei.bucket/scratch/samplePCADat.ubuntu.20140526.192324.390970/output/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "removing tmp directory /tmp/samplePCADat.ubuntu.20140526.192324.390970\r\n",
        "Removing all files in s3://weiwei.bucket/scratch/samplePCADat.ubuntu.20140526.192324.390970/\r\n"
       ]
      }
     ],
     "prompt_number": 373
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile stnDistSample.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "Get the weight distribution of stations in sample_dat.csv\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "from sys import stderr\n",
      "import re,pickle,base64,zlib\n",
      "\n",
      "def decode(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n",
      "\n",
      "def encode(Value):\n",
      "    \"\"\" Encode a value as a string \"\"\"\n",
      "    return base64.b64encode(zlib.compress(pickle.dumps(Value),9))\n",
      "\n",
      "class stnDistSample(MRJob):\n",
      "    def pca_prescreen_mapper(self,_,line):\n",
      "        try:\n",
      "            stn,trecEn = line.split(\"\\t\")\n",
      "            stn=stn.strip('\\\"')\n",
      "            #trecEn=trecEn.strip('\\\"')\n",
      "            yield stn,1\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+str(line[:5]))\n",
      "            stderr.write(str(e))\n",
      "    def pca_prescreen_reducer(self,key,value):\n",
      "        try:\n",
      "            outkey=key\n",
      "            outval=sum(list(value))\n",
      "            if outval>5:\n",
      "                yield outkey,outval\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e)+\"\\t\")\n",
      "            \n",
      "    def steps(self):\n",
      "        return [\n",
      "            self.mr(mapper=self.pca_prescreen_mapper,\n",
      "                    reducer=self.pca_prescreen_reducer)\n",
      "        ]\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    stnDistSample.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting stnDistSample.py\n"
       ]
      }
     ],
     "prompt_number": 383
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!python stnDistSample.py -r local sample_dat_10k.csv > stnWtDistSample.csv\n",
      "#!python stnDistSample.py -r emr --emr-job-flow-id  $job_flow_id s3://weiwei.bucket/data/sample_dat.csv/ > stnWtDistSample.csv\n",
      "#--output-dir  s3://weiwei.bucket/data/sample_dat.csv/ --no-output  \n",
      "# j-31UKS93V80CN7"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 385
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile samplePCA.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "run PCA on a single node, all data from one station\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "from sys import stderr\n",
      "import pandas as pd\n",
      "import re,pickle,base64,zlib\n",
      "import numpy as np\n",
      "\n",
      "def decode(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n",
      "\n",
      "def encode(Value):\n",
      "    \"\"\" Encode a value as a string \"\"\"\n",
      "    return base64.b64encode(zlib.compress(pickle.dumps(Value),9))\n",
      "\n",
      "def checkThreshold(x):\n",
      "    x=list(x)\n",
      "    stderr.write(str(x[:10]))\n",
      "    for i in x:\n",
      "        if i > 0.99:\n",
      "            return x.index(i)\n",
      "\n",
      "class mrpca(MRJob):\n",
      "    def NN(self,vec):\n",
      "        \"\"\"nearest neighbor method to substitute missing values in record the nearest left neighbor\"\"\"\n",
      "        # if there is None\n",
      "        if not None in vec:\n",
      "            return vec\n",
      "        else:\n",
      "#             stderr.write(\"type of vector \"+\"\\t\"+str(type(vec)))\n",
      "            for i in xrange(len(vec)):\n",
      "                if vec[i]==None:\n",
      "                    if i>0:\n",
      "                        vec[i]=vec[i-1]\n",
      "                    else:\n",
      "                        vec[i]=filter(None,vec)[0]\n",
      "\n",
      "            return vec\n",
      "            \n",
      "    def str2flt(self,vec):\n",
      "        \"\"\"convert string elements to float; missing values are replaced with None\"\"\"\n",
      "        newvec=[]\n",
      "        for v in vec:\n",
      "            try:\n",
      "                newv=float(v)\n",
      "            except:\n",
      "                newv=None\n",
      "            finally:\n",
      "                newvec.append(newv)\n",
      "        return newvec\n",
      "                \n",
      "    def pca_dat_prep(self, _, line):\n",
      "        # experiment with \"USC00120177\"   7 records\n",
      "        try:\n",
      "            stn,trecEn = line.split(\"\\t\")\n",
      "            stn=stn.strip('\\\"')\n",
      "            if stn==\"USC00120177\":\n",
      "                trec=decode(trecEn.strip('\\\"'))\n",
      "                # array element type str->float, drop NaN and substitude the value with nearest neighbor\n",
      "                tmax = self.NN(self.str2flt(trec[\"TMAX\"]))\n",
      "                tmin = self.NN(self.str2flt(trec[\"TMIN\"]))\n",
      "                if None in tmax or None in tmin:\n",
      "                    stderr.write(\"None exists\\n\")\n",
      "#                 stderr.write(\"tmax: \"+\"\\t\"+str(tmax)+\"\\n\") # \n",
      "                datArr = np.array([tmax,tmin]).T\n",
      "#                 stderr.write(\"dataArr \"+str(datArr.shape)+\"\\n\")\n",
      "                yield stn, encode(datArr)\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+str(line[:5]))\n",
      "            stderr.write(str(e))\n",
      "\n",
      "    def pca_indi_sum(self,key,value):\n",
      "        '''get the sum of measurement 2-D numpy arrays'''\n",
      "        try:\n",
      "            stn=key\n",
      "            vals=list(value)\n",
      "            arrays = [decode(x) for x in vals]#x = x[~numpy.isnan(x)] # remove nan\n",
      "            measSum=np.zeros(shape=arrays[0].shape) # neasSum type: numpy array\n",
      "#             stderr.write(\"measSum initialized\\n\")\n",
      "            for arr in arrays:\n",
      "                measSum+=arr\n",
      "            yield stn,(encode(measSum),encode(len(arrays)),encode(arrays)) # sum of arrays\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e)+\"\\t\")\n",
      "            \n",
      "    def pca_indi_mean(self,key,value):\n",
      "        '''calculate the mean vector of this node/station'''\n",
      "        try:\n",
      "            stn=key\n",
      "            vals=[v for v in value]\n",
      "            measSumEn,countEn,arraysEn = vals[0]\n",
      "            measSum = decode(measSumEn)\n",
      "            count=decode(countEn)\n",
      "            mean = measSum/float(count)\n",
      "#             stderr.write(stn+\"\\t\"+str(mean))\n",
      "            yield stn,(encode(mean),arraysEn)\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e)+\"\\t\")    \n",
      "\n",
      "    \n",
      "    def pca_indi_cov(self,key,value):\n",
      "        \"calculate variance and covariance matrix\"\n",
      "        try:\n",
      "            stn=key\n",
      "            vals=list(value)\n",
      "            meanEn,arraysEn = vals[0]\n",
      "            mean=decode(meanEn)\n",
      "            arrays=decode(arraysEn) # shape of each array (365,2)\n",
      "            cov=[]\n",
      "            for arr in arrays: # remove [:2], this is just for testing\n",
      "                v=arr-mean# variance array\n",
      "                cov.append(np.dot(v,v.T)) ## Here should use np.dot rather than np.outer. The np definition is confusing. \n",
      "                                          ## if np.outer, the dimension is incorrect. shuld be 365*365\n",
      "#             stderr.write(\"shape of cov[0]: \"+\"\\t\"+str(cov[0].shape))\n",
      "            yield stn,encode(cov)\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e)+\"\\n\")\n",
      "            \n",
      "    def pca_indi_svd(self,key,value):\n",
      "        \"\"\"get eigen vectors explaining 99% of variance\"\"\"\n",
      "        try:\n",
      "            stn=key\n",
      "            vals=list(value)\n",
      "            covArrEn = vals[0] # encoded covariance arrays\n",
      "#             stderr.write(\"cov: \"+\"\\t\"+str(len(decode(covArrEn)))+\"\\n\"+str(decode(covArrEn)[:5])+\"\\n\")\n",
      "            covArr = decode(covArrEn)\n",
      "            covSum = np.zeros(covArr[0].shape)\n",
      "            for cov in covArr:\n",
      "                covSum+=cov\n",
      "            meanCov = covSum/float(len(covArr))\n",
      "            ## SVD below\n",
      "            U,D,V=np.linalg.svd(cov)\n",
      "            varXpln = np.cumsum(D[:])/np.sum(D) # cummulative density of explained variance\n",
      "#             stderr.write(\"var explained: \\n\"+str(varXpln))\n",
      "#             stderr.write(\"shape of U: \"+\"\\t\"+str(U.shape))\n",
      "#             stderr.write(\"shape of D: \"+\"\\t\"+str(D.shape))\n",
      "#             stderr.write(\"shape of V: \"+\"\\t\"+str(V.shape))\n",
      "            ## eigen vector selection, need explain 99% variance\n",
      "            cut=checkThreshold(varXpln)\n",
      "#             stderr.write(\"cut: \"+'\\n'+str(cut))\n",
      "            ev = U[:,:cut] ## selected eigen vectors\n",
      "#             stderr.write(\"Results: \\n\"+str(ev.shape)+\"\\n\"+str(ev)+'\\n')\n",
      "            yield stn,encode(ev)\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e)+\"\\n\")\n",
      "            \n",
      "    def steps(self):\n",
      "        return [\n",
      "            self.mr(mapper=self.pca_dat_prep,\n",
      "                    reducer=self.pca_indi_sum),\n",
      "            self.mr(reducer=self.pca_indi_mean),\n",
      "            self.mr(reducer=self.pca_indi_cov),\n",
      "            self.mr(reducer=self.pca_indi_svd)\n",
      "            \n",
      "        ]\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    mrpca.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting samplePCA.py\n"
       ]
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python samplePCA.py -r emr --emr-job-flow-id  $job_flow_id sample_dat_10k.csv > samplePCA.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /tmp/samplePCA.ubuntu.20140527.010045.554912\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://weiwei.bucket/scratch/samplePCA.ubuntu.20140527.010045.554912/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow j-31UKS93V80CN7\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 30.8s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>81cf5826-e53a-11e3-977d-81d1fc5a197d</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>8dc071b0-e53a-11e3-8c68-dfb11431f3fe</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 30.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 111.5s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 142.3s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 173.2s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 203.9s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 234.7s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 265.5s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 296.3s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 327.1s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 358.0s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 388.7s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>572dfe6e-e53b-11e3-9423-c97367f9860b</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 439.5s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 470.3s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 501.0s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>9a231b92-e53b-11e3-b43a-872b56d6dfca</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>a614a9de-e53b-11e3-9423-c97367f9860b</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 30.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 581.9s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 612.7s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 643.5s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 674.3s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 705.2s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 736.0s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 766.7s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 797.5s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 828.3s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 859.1s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 889.8s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 920.6s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 951.4s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 982.3s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1013.1s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1043.9s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1074.7s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>f00ba41f-e53c-11e3-955c-1fb30f4201c5</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1125.5s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1156.4s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1187.2s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1218.0s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1248.7s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1279.5s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1316.6s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1347.4s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1378.3s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1409.1s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1440.0s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1470.9s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1501.7s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1532.6s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>0101e06b-e53e-11e3-9423-c97367f9860b</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1583.4s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1614.2s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1644.9s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1675.7s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1706.5s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1737.4s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1768.2s ago, status RUNNING: Running step\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job on job flow j-31UKS93V80CN7 failed with status WAITING: Waiting after step failed\r\n",
        "Logs are in s3://yoav.hadoop/log/j-31UKS93V80CN7/\r\n",
        "ec2_key_pair_file not specified, going to S3\r\n",
        "Scanning S3 logs for probable cause of failure\r\n",
        "Waiting 5.0s for S3 eventual consistency\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>90e0eeb9-e53e-11e3-a7f3-054eed488613</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Attempting to terminate job...\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Traceback (most recent call last):\r\n",
        "  File \"samplePCA.py\", line 167, in <module>\r\n",
        "    mrpca.run()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/job.py\", line 494, in run\r\n",
        "    mr_job.execute()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/job.py\", line 512, in execute\r\n",
        "    super(MRJob, self).execute()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/launch.py\", line 147, in execute\r\n",
        "    self.run_job()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/launch.py\", line 213, in run_job\r\n",
        "    self.stdout.flush()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/runner.py\", line 614, in __exit__\r\n",
        "    self.cleanup()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/emr.py\", line 1010, in cleanup\r\n",
        "    super(EMRJobRunner, self).cleanup(mode=mode)\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/runner.py\", line 560, in cleanup\r\n",
        "    self._cleanup_job()\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/emr.py\", line 1084, in _cleanup_job\r\n",
        "    self._opts['ec2_key_pair_file'])\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/ssh.py\", line 200, in ssh_terminate_single_job\r\n",
        "    ssh_bin, address, ec2_key_pair_file, ['hadoop', 'job', '-list']))\r\n",
        "  File \"/usr/local/lib/python2.7/dist-packages/mrjob/ssh.py\", line 82, in ssh_run\r\n",
        "    p = Popen(args, stdout=PIPE, stderr=PIPE, stdin=PIPE)\r\n",
        "  File \"/home/ubuntu/anaconda/lib/python2.7/subprocess.py\", line 709, in __init__\r\n",
        "    errread, errwrite)\r\n",
        "  File \"/home/ubuntu/anaconda/lib/python2.7/subprocess.py\", line 1326, in _execute_child\r\n",
        "    raise child_exception\r\n",
        "TypeError: execv() arg 2 must contain only strings\r\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile weatherPCA.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "run PCA on all nodes, all data from one station\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "from sys import stderr\n",
      "import pandas as pd\n",
      "import re,pickle,base64,zlib\n",
      "import numpy as np\n",
      "\n",
      "def decode(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n",
      "\n",
      "def encode(Value):\n",
      "    \"\"\" Encode a value as a string \"\"\"\n",
      "    return base64.b64encode(zlib.compress(pickle.dumps(Value),9))\n",
      "\n",
      "def checkThreshold(x):\n",
      "    x=list(x)\n",
      "    stderr.write(str(x[:10]))\n",
      "    for i in x:\n",
      "        if i > 0.99:\n",
      "            return x.index(i)\n",
      "\n",
      "class weatherPCA(MRJob):\n",
      "    def NN(self,vec):\n",
      "        \"\"\"nearest neighbor method to substitute missing values in record the nearest left neighbor\"\"\"\n",
      "        # if there is None\n",
      "        if not None in vec:\n",
      "            return vec\n",
      "        else:\n",
      "#             stderr.write(\"type of vector \"+\"\\t\"+str(type(vec)))\n",
      "            for i in xrange(len(vec)):\n",
      "                if vec[i]==None:\n",
      "                    if i>0:\n",
      "                        vec[i]=vec[i-1]\n",
      "                    else:\n",
      "                        vec[i]=filter(None,vec)[0]\n",
      "\n",
      "            return vec\n",
      "            \n",
      "    def str2flt(self,vec):\n",
      "        \"\"\"convert string elements to float; missing values are replaced with None\"\"\"\n",
      "        newvec=[]\n",
      "        for v in vec:\n",
      "            try:\n",
      "                newv=float(v)\n",
      "            except:\n",
      "                newv=None\n",
      "            finally:\n",
      "                newvec.append(newv)\n",
      "        return newvec\n",
      "                \n",
      "    def pca_dat_prep(self, _, line):\n",
      "        '''\n",
      "        Input is a station and the records of tmax and tmin of some year.\\\n",
      "        Output is station and encoded 365*2 numpy arrays\n",
      "        '''\n",
      "        try:\n",
      "            stn,trecEn = line.split(\"\\t\")\n",
      "            stn=stn.strip('\\\"')\n",
      "            trec=decode(trecEn.strip('\\\"'))\n",
      "            # array element type str->float, drop NaN and substitude the value with nearest neighbor\n",
      "            tmax = self.NN(self.str2flt(trec[\"TMAX\"]))\n",
      "            tmin = self.NN(self.str2flt(trec[\"TMIN\"]))\n",
      "            # for double check\n",
      "            if None in tmax or None in tmin:\n",
      "                stderr.write(\"None exists\\n\")\n",
      "#           stderr.write(\"tmax: \"+\"\\t\"+str(tmax)+\"\\n\") # \n",
      "            datArr = np.array([tmax,tmin]).T\n",
      "#           stderr.write(\"dataArr \"+str(datArr.shape)+\"\\n\")\n",
      "            yield stn, encode(datArr)\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+str(line[:5]))\n",
      "            stderr.write(str(e))\n",
      "\n",
      "    def pca_indi_sum(self,key,value):\n",
      "        '''get the sum of measurement 2-D numpy arrays'''\n",
      "        try:\n",
      "            stn=key\n",
      "            vals=list(value)\n",
      "            arrays = [decode(x) for x in vals]#x = x[~numpy.isnan(x)] # remove nan\n",
      "            measSum=np.zeros(shape=arrays[0].shape) # neasSum type: numpy array\n",
      "#             stderr.write(\"measSum initialized\\n\")\n",
      "            for arr in arrays:\n",
      "                measSum+=arr\n",
      "            yield stn,(encode(measSum),encode(len(arrays)),encode(arrays)) # sum of arrays\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e)+\"\\t\")\n",
      "            \n",
      "    def pca_indi_mean(self,key,value):\n",
      "        '''calculate the mean vector of this node/station'''\n",
      "        try:\n",
      "            stn=key\n",
      "            vals=[v for v in value]\n",
      "            measSumEn,countEn,arraysEn = vals[0]\n",
      "            measSum = decode(measSumEn)\n",
      "            count=decode(countEn)\n",
      "            mean = measSum/float(count)\n",
      "#             stderr.write(stn+\"\\t\"+str(mean))\n",
      "            yield stn,(encode(mean),arraysEn)\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e)+\"\\t\")    \n",
      "\n",
      "    \n",
      "    def pca_indi_cov(self,key,value):\n",
      "        \"calculate variance and covariance matrix\"\n",
      "        try:\n",
      "            stn=key\n",
      "            vals=list(value)\n",
      "            meanEn,arraysEn = vals[0]\n",
      "            mean=decode(meanEn)\n",
      "            arrays=decode(arraysEn) # shape of each array (365,2)\n",
      "            cov=[]\n",
      "            for arr in arrays: # remove [:2], this is just for testing\n",
      "                v=arr-mean# variance array\n",
      "                cov.append(np.dot(v,v.T)) ## Here should use np.dot rather than np.outer. The np definition is confusing. \n",
      "                                          ## if np.outer, the dimension is incorrect. shuld be 365*365\n",
      "#             stderr.write(\"shape of cov[0]: \"+\"\\t\"+str(cov[0].shape))\n",
      "            yield stn,encode(cov)\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e)+\"\\n\")\n",
      "            \n",
      "    def pca_indi_svd(self,key,value):\n",
      "        \"\"\"get eigen vectors explaining 99% of variance\"\"\"\n",
      "        try:\n",
      "            stn=key\n",
      "            vals=list(value)\n",
      "            covArrEn = vals[0] # encoded covariance arrays\n",
      "#             stderr.write(\"cov: \"+\"\\t\"+str(len(decode(covArrEn)))+\"\\n\"+str(decode(covArrEn)[:5])+\"\\n\")\n",
      "            covArr = decode(covArrEn)\n",
      "            covSum = np.zeros(covArr[0].shape)\n",
      "            for cov in covArr:\n",
      "                covSum+=cov\n",
      "            meanCov = covSum/float(len(covArr))\n",
      "            ## SVD below\n",
      "            U,D,V=np.linalg.svd(cov)\n",
      "            varXpln = np.cumsum(D[:])/np.sum(D) # cummulative density of explained variance\n",
      "#             stderr.write(\"var explained: \\n\"+str(varXpln))\n",
      "#             stderr.write(\"shape of U: \"+\"\\t\"+str(U.shape))\n",
      "#             stderr.write(\"shape of D: \"+\"\\t\"+str(D.shape))\n",
      "#             stderr.write(\"shape of V: \"+\"\\t\"+str(V.shape))\n",
      "            ## eigen vector selection, need explain 99% variance\n",
      "            cut=checkThreshold(varXpln)\n",
      "#             stderr.write(\"cut: \"+'\\n'+str(cut))\n",
      "            ev = U[:,:cut] ## selected eigen vectors\n",
      "#             stderr.write(\"Results: \\n\"+str(ev.shape)+\"\\n\"+str(ev)+'\\n')\n",
      "            yield stn,encode(ev)\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e)+\"\\n\")\n",
      "            \n",
      "    def steps(self):\n",
      "        return [\n",
      "            self.mr(mapper=self.pca_dat_prep,\n",
      "                    reducer=self.pca_indi_sum),\n",
      "            self.mr(reducer=self.pca_indi_mean),\n",
      "            self.mr(reducer=self.pca_indi_cov),\n",
      "            self.mr(reducer=self.pca_indi_svd)\n",
      "            \n",
      "        ]\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    weatherPCA.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing weatherPCA.py\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python weatherPCA.py -r emr --emr-job-flow-id  $job_flow_id sample_dat_10k.csv > samplePCA_allnodes.csv\n",
      "## completed\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "using configs in /home/ubuntu/.mrjob.conf\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "creating tmp directory /tmp/weatherPCA.ubuntu.20140527.013707.958277\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Copying non-input files into s3://weiwei.bucket/scratch/weatherPCA.ubuntu.20140527.013707.958277/files/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Adding our job to existing job flow j-31UKS93V80CN7\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 30.9s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 1 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 61.7s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 1 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 92.6s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 1 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 123.4s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 2 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>ce402f65-e53f-11e3-80e8-ff6cff06198c</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 174.3s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 2 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>ec93dbb4-e53f-11e3-bea8-dbf80bac8b69</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 225.1s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 255.9s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 286.7s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 317.5s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 348.3s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 379.1s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 410.4s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 441.2s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 472.0s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 502.8s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 533.6s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 564.5s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 595.3s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 626.2s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 657.0s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 687.9s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>1ebcc71a-e541-11e3-8362-0f887c06ae20</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>2aadb993-e541-11e3-af71-7d8b78bb4770</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 30.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 768.9s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 799.7s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 830.5s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>73bda54c-e541-11e3-acfe-a31808eb12d5</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>7fb15671-e541-11e3-a7f3-054eed488613</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 30.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 911.5s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 942.4s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 973.3s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>c8dae368-e541-11e3-a7f3-054eed488613</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1024.2s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>e73371d6-e541-11e3-81ac-09de5656b974</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>f324d8b1-e541-11e3-acfe-a31808eb12d5</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 30.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1105.2s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1136.0s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1166.9s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1197.7s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1228.6s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1259.4s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>735c555a-e542-11e3-977d-81d1fc5a197d</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>7f4d473d-e542-11e3-a13b-cf267f9d23d6</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 30.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>913579e2-e542-11e3-9fde-05f53a330f8a</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 45.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1385.4s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1416.2s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1447.0s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1477.9s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>f594838e-e542-11e3-aa87-172672d76e00</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1528.8s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 3 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>13eceac4-e543-11e3-9077-5114ea9dc6a3</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>1fe111b8-e543-11e3-9077-5114ea9dc6a3</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 30.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>31c9e03c-e543-11e3-aa87-172672d76e00</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 45.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1654.8s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 4 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Got retriable error: EmrResponseError: 400 Bad Request\r\n",
        "<ErrorResponse xmlns=\"http://elasticmapreduce.amazonaws.com/doc/2009-03-31\">\r\n",
        "  <Error>\r\n",
        "    <Type>Sender</Type>\r\n",
        "    <Code>Throttling</Code>\r\n",
        "    <Message>Rate exceeded</Message>\r\n",
        "  </Error>\r\n",
        "  <RequestId>5f0c5578-e543-11e3-9423-c97367f9860b</RequestId>\r\n",
        "</ErrorResponse>\r\n",
        "\r\n",
        "Backing off for 20.0 seconds\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1705.7s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 4 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1736.5s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 4 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1767.3s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 4 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1798.1s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 4 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1828.9s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 4 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1859.8s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 4 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1890.7s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 4 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1921.6s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 4 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1952.5s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 4 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 1983.3s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 4 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 2014.2s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 4 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job launched 2045.0s ago, status RUNNING: Running step (weatherPCA.ubuntu.20140527.013707.958277: Step 4 of 4)\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Job completed.\r\n",
        "Running time was 2037.0s (not counting time spent waiting for the EC2 instances)\r\n",
        "ec2_key_pair_file not specified, going to S3\r\n",
        "Fetching counters from S3...\r\n",
        "Waiting 5.0s for S3 eventual consistency\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Counters may not have been uploaded to S3 yet. Try again in 5 minutes with: mrjob fetch-logs --counters j-31UKS93V80CN7\r\n",
        "Counters from step 1:\r\n",
        "  (no counters found)\r\n",
        "Counters from step 2:\r\n",
        "  (no counters found)\r\n",
        "Counters from step 3:\r\n",
        "  (no counters found)\r\n",
        "Counters from step 4:\r\n",
        "  (no counters found)\r\n",
        "Streaming final output from s3://weiwei.bucket/scratch/weatherPCA.ubuntu.20140527.013707.958277/output/\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "removing tmp directory /tmp/weatherPCA.ubuntu.20140527.013707.958277\r\n",
        "Removing all files in s3://weiwei.bucket/scratch/weatherPCA.ubuntu.20140527.013707.958277/\r\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<span style=\"color:blue\" > Merge nodes according to eigen vector number</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile mergeStns.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "Merge neighbor nodes according to the number of selected eigen vectors\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "from sys import stderr\n",
      "import pandas as pd\n",
      "import re,pickle,base64,zlib\n",
      "import numpy as np\n",
      "\n",
      "def decode(eVal):\n",
      "    \"\"\" Decode a string into a value \"\"\"\n",
      "    return pickle.loads(zlib.decompress(base64.b64decode(eVal)))\n",
      "\n",
      "def encode(Value):\n",
      "    \"\"\" Encode a value as a string \"\"\"\n",
      "    return base64.b64encode(zlib.compress(pickle.dumps(Value),9))\n",
      "\n",
      "class mergeStnsTest(MRJob):\n",
      "    '''\n",
      "    List all stations belonging to one node, such as 000000000\n",
      "    calculate every pair of stations and exame the description length.\n",
      "    If the description length after merging is smaller than the sum of the number \n",
      "    of original eigen vectors, the two stations will be merged. \n",
      "    This can be generalized to merging nodes.\n",
      "    '''\n",
      "    def dat_prep_mapper(self, _, line):\n",
      "        '''\n",
      "        Take node 000000000 as an example, find all stations belong to this node; retrieve TMAX/TMIN data of these nodes;\n",
      "        retrieve eigen vectors of these nodes;\n",
      "        '''\n",
      "        try:\n",
      "            ## use Professor's prebuilt tree\n",
      "            tree = pickle.load(open('/home/ubuntu/UCSD_BigData/data/weather/tree.pkl'))\n",
      "            ptree = tree['Partition_Tree']\n",
      "            pstn = tree['Partitioned_Stations']\n",
      "            pstnSample=pstn[pstn['Node']=='000000000']\n",
      "            pstnSample.sort(['latitude','longitude']) # sort by both latitude and longitude, latitude prior\n",
      "            \n",
      "            ## select the first two stations\n",
      "            stn_1 = pstnSample.index[0]\n",
      "            stn_1\n",
      "            stn,trecEn = line.split(\"\\t\")\n",
      "            stn=stn.strip('\\\"')\n",
      "            trec=decode(trecEn.strip('\\\"'))\n",
      "            # array element type str->float, drop NaN and substitude the value with nearest neighbor\n",
      "            tmax = self.NN(self.str2flt(trec[\"TMAX\"]))\n",
      "            tmin = self.NN(self.str2flt(trec[\"TMIN\"]))\n",
      "            # for double check\n",
      "            if None in tmax or None in tmin:\n",
      "                stderr.write(\"None exists\\n\")\n",
      "#           stderr.write(\"tmax: \"+\"\\t\"+str(tmax)+\"\\n\") # \n",
      "            datArr = np.array([tmax,tmin]).T\n",
      "#           stderr.write(\"dataArr \"+str(datArr.shape)+\"\\n\")\n",
      "            yield stn, encode(datArr)\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+str(line[:5]))\n",
      "            stderr.write(str(e))\n",
      "\n",
      "    def pca_indi_sum(self,key,value):\n",
      "        '''get the sum of measurement 2-D numpy arrays'''\n",
      "        try:\n",
      "            stn=key\n",
      "            vals=list(value)\n",
      "            arrays = [decode(x) for x in vals]#x = x[~numpy.isnan(x)] # remove nan\n",
      "            measSum=np.zeros(shape=arrays[0].shape) # neasSum type: numpy array\n",
      "#             stderr.write(\"measSum initialized\\n\")\n",
      "            for arr in arrays:\n",
      "                measSum+=arr\n",
      "            yield stn,(encode(measSum),encode(len(arrays)),encode(arrays)) # sum of arrays\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e)+\"\\t\")\n",
      "            \n",
      "    def pca_indi_mean(self,key,value):\n",
      "        '''calculate the mean vector of this node/station'''\n",
      "        try:\n",
      "            stn=key\n",
      "            vals=[v for v in value]\n",
      "            measSumEn,countEn,arraysEn = vals[0]\n",
      "            measSum = decode(measSumEn)\n",
      "            count=decode(countEn)\n",
      "            mean = measSum/float(count)\n",
      "#             stderr.write(stn+\"\\t\"+str(mean))\n",
      "            yield stn,(encode(mean),arraysEn)\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e)+\"\\t\")    \n",
      "\n",
      "    \n",
      "    def pca_indi_cov(self,key,value):\n",
      "        \"calculate variance and covariance matrix\"\n",
      "        try:\n",
      "            stn=key\n",
      "            vals=list(value)\n",
      "            meanEn,arraysEn = vals[0]\n",
      "            mean=decode(meanEn)\n",
      "            arrays=decode(arraysEn) # shape of each array (365,2)\n",
      "            cov=[]\n",
      "            for arr in arrays: # remove [:2], this is just for testing\n",
      "                v=arr-mean# variance array\n",
      "                cov.append(np.dot(v,v.T)) ## Here should use np.dot rather than np.outer. The np definition is confusing. \n",
      "                                          ## if np.outer, the dimension is incorrect. shuld be 365*365\n",
      "#             stderr.write(\"shape of cov[0]: \"+\"\\t\"+str(cov[0].shape))\n",
      "            yield stn,encode(cov)\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e)+\"\\n\")\n",
      "            \n",
      "    def pca_indi_svd(self,key,value):\n",
      "        \"\"\"get eigen vectors explaining 99% of variance\"\"\"\n",
      "        try:\n",
      "            stn=key\n",
      "            vals=list(value)\n",
      "            covArrEn = vals[0] # encoded covariance arrays\n",
      "#             stderr.write(\"cov: \"+\"\\t\"+str(len(decode(covArrEn)))+\"\\n\"+str(decode(covArrEn)[:5])+\"\\n\")\n",
      "            covArr = decode(covArrEn)\n",
      "            covSum = np.zeros(covArr[0].shape)\n",
      "            for cov in covArr:\n",
      "                covSum+=cov\n",
      "            meanCov = covSum/float(len(covArr))\n",
      "            ## SVD below\n",
      "            U,D,V=np.linalg.svd(cov)\n",
      "            varXpln = np.cumsum(D[:])/np.sum(D) # cummulative density of explained variance\n",
      "#             stderr.write(\"var explained: \\n\"+str(varXpln))\n",
      "#             stderr.write(\"shape of U: \"+\"\\t\"+str(U.shape))\n",
      "#             stderr.write(\"shape of D: \"+\"\\t\"+str(D.shape))\n",
      "#             stderr.write(\"shape of V: \"+\"\\t\"+str(V.shape))\n",
      "            ## eigen vector selection, need explain 99% variance\n",
      "            cut=checkThreshold(varXpln)\n",
      "#             stderr.write(\"cut: \"+'\\n'+str(cut))\n",
      "            ev = U[:,:cut] ## selected eigen vectors\n",
      "#             stderr.write(\"Results: \\n\"+str(ev.shape)+\"\\n\"+str(ev)+'\\n')\n",
      "            yield stn,encode(ev)\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e)+\"\\n\")\n",
      "            \n",
      "    def steps(self):\n",
      "        return [\n",
      "            self.mr(mapper=self.pca_dat_prep,\n",
      "                    reducer=self.pca_indi_sum),\n",
      "            self.mr(reducer=self.pca_indi_mean),\n",
      "            self.mr(reducer=self.pca_indi_cov),\n",
      "            self.mr(reducer=self.pca_indi_svd)\n",
      "            \n",
      "        ]\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    weatherPCA.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!less samplePCA_allnodes.csv"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u001b7\u001b[?47h\u001b[?1h\u001b=\r",
        "\"AR000087270\"   \"eNrs2r1qAkEUgNH+vog2kVmNxoCQ2sYitoGgq0Ig6rAZIb59Zk3KpEt5irMsOz8 \bLt/7a0+WYr6P23O1Hx8t7edt03eYar92+PZ8+SndpS+QUw/a2L0677/XcxHCZouRxrAfbQeRJfb+P5zy \bt3/u1yWwat0fJs/g5vCvXvI/8EMP14DCvh+ZR76jbS36sR5vUn6v3LepS08RqtVreNXHT/6oZx3aZUt3 \bw8pnS7w7p6e9FAAAAAAAAAAAAAAD+h3gbAAAAAAAAAAAAAEC8DQAAAAAAAAAAAAAg3gYAAAAAAAAAAAA \bAQLwNAAAAAAAAAAAAACDeBgAAAAAAAAAAAAAQbwMAAAAAAAAAAAAAIN4GAAAAAAAAAAAAABBvAwAAAAA \bAAAAAAAAg3gYAAAAAAAAAAAAAEG8DAAAAAAAAAAAAAIi3AQAAAAAAAAAAAAAQbwMAAAAAAAAAAAAAiLc \bBAAAAAAAAAAAAAMTbhgAAAAAAAAAAAAAAIN4GAAAAAAAAAAAAABBvAwAAAAAAAAAAAAAg3gYAAAAAAAA \bAAAAAEG8DAAAAAAAAAAAAAIi3AQAAAAAAAAAAAAAQbwMAAAAAAAAAAAAAiLcBAAAAAAAAAAAAABBvAwA \bAAAAAAAAAAACItwEAAAAAAAAAAAAAxNsAAAAAAAAAAAAAAIi3AQAAAAAAAAAAAADE2wAAAAAAAAAAAAA \bA4m2DAAAAAAAAAAAAAAAQbwMAAAAAAAAAAAAAiLcBAAAAAAAAAAAAABBvAwAAAAAAAAAAAACItwEAAAA \bAAAAAAAAAxNsAAAAAAAAAAAAAAIi3AQAAAAAAAAAAAADE2wAAAAAAAAAAAAAAiLcBAAAAAAAAAAAAAMT \bbAAAAAAAAAAAAAADibQAAAAAAAAAAAAAAxNsAAAAAAAAAAAAAAOJtAAAAAAAAAAAAAADxNgAAAAAAAAA \bAAAAA4m0AAAAAAAAAAAAAAPE2AAAAAAAAAAAAAADibQAAAAAAAAAAAAAA8TYAAAAAAAAAAAAAgHgbAAA \bAAAAAAAAAAADxNgAAAAAAAAAAAACAeBsAAAAAAAAAAAAAAPE2AAAAAAAAAAAAAIB4GwAAAAAAAAAAAAB \bAvA0AAAAAAAAAAAAAgHgbAAAAAAAAAAAAAEC8DQAAAAAAAAAAAAAg3gYAAAAAAAAAAAAAQLwNAAAAAAA \bAAAAAACDeBgAAAAAAAAAAAABAvA0AAAAAAAAAAAAAIN4GAAAAAAAAAAAAABBvAwAAAAAAAAAAAAAg3gY \bAAAAAAAAAAAAAEG8DAAAAAAAAAAAAACDeBgAAAAAAAAAAAAAQbwMAAAAAAAAAAAAAiLcBAAAAAAAAAAA \bAABBvAwAAAAAAAAAAAACItwEAAAAAAAAAAAAAxNsAAAAAAAAAAAAAAIi3AQAAAAAAAAAAAADE2wAAAAA \bAAAAAAAAAiLcBAAAAAAAAAAAAAMTbAAAAAAAAAAAAAADibQAAAAAAAAAAAAAAxNsAAAAAAAAAAAAAAOJ \btAAAAAAAAAAAAAADE2wAAAAAAAAAAAAAA4m0AAAAAAAAAAAAAAPE2AAAAAAAAAAAAAADibQAAAAAAAAA \bAAAAA8TYAAAAAAAAAAAAAgHgbAAAAAAAAAAAAAADxNgAAAAAAAAAAAACAeBsAAAAAAAAAAAAAAPE2AAA \bAAAAAAAAAAIB4GwAAAAAAAAAAAABAvA0AAAAAAAAAAAAAgHgbAAAAAAAAAAAAAEC8DQAAAAAAAAAAAAC \bAeBsAAAAAAAAAAAAAQLwNAAAAAAAAAAAAACDeBgAAAAAAAAAAAABAvA0AAAAAAAAAAAAAIN4GAAAAAAA \b\u001b[7msamplePCA_allnodes.csv\u001b[m\u001b[K"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\u0007\r",
        "\u001b[K:\u001b[K\r",
        "\u001b[KAAAAAABBvAwAAAAAAAAAAAAAg3gYAAAAAAAAAAAAAEG8DAAAAAAAAAAAAACDeBgAAAAAAAAAAAAAQbwM \b:\u001b[K"
       ]
      }
     ],
     "prompt_number": 45
    }
   ],
   "metadata": {}
  }
 ]
}