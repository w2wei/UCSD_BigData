{
 "metadata": {
  "name": "",
  "signature": "sha256:4e864a82a056a43861ff887567fcced133c892e1de3992ad87414bf61e4d8484"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "import sklearn as sk\n",
      "import sys\n",
      "home_dir='/home/ubuntu/UCSD_BigData'\n",
      "dat_dir='/home/ubuntu/UCSD_BigData/data/weather'\n",
      "sys.path.append(home_dir+'/utils')\n",
      "from find_waiting_flow import *\n",
      "from AWS_keypair_management import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "localData=home_dir+'/data/weather/ALL.head.csv'\n",
      "#!cat $localData"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 172
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "station=dat_dir+'/ghcnd-stations.txt'\n",
      "#!less $station"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "readme = dat_dir+'/ghcnd-readme.txt'\n",
      "#!cat $readme"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile element_dist.py\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "summarize the distribution of measurements\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "from sys import stderr\n",
      "\n",
      "class measDist(MRJob):\n",
      "\n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            elements=line.split(',')\n",
      "            if elements[0]=='station':\n",
      "                out=('header',1)\n",
      "            else:\n",
      "                out=(elements[1],1)\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+line)\n",
      "            stderr.write(e)\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "            out=('error',1)\n",
      "\n",
      "        finally:\n",
      "            yield out\n",
      "            \n",
      "    def combiner(self, word, counts):\n",
      "        self.increment_counter('MrJob Counters','combiner',1)\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "    def reducer(self, word, counts):\n",
      "        self.increment_counter('MrJob Counters','reducer',1)\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    measDist.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting element_dist.py\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "Creds= pickle.load(open('/home/ubuntu/Vault/Creds.pkl','rb'))\n",
      "pair=Creds['mrjob']\n",
      "key_id=pair['key_id']\n",
      "secret_key=pair['secret_key']\n",
      "ID=pair['ID']\n",
      "print ID,key_id, secret_key\n",
      "job_flow_id=find_waiting_flow(key_id,secret_key)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "772502224841 AKIAI7UVDNTQ7W6VIMXA VuYQTXnihQ9JP68uwnRz1xD50OA6CSY+Wm/bNqNA\n",
        "<boto.emr.emrobject.JobFlow object at 0x3c2da10>"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " no_script.yoavfreund.20140516.040032.370095 j-262J0JTFJIRLO WAITING\n",
        "<boto.emr.emrobject.JobFlow object at 0x4132850> no_script.yoavfreund.20140517.080731.371759 j-1RE8D7HBISOI0 WAITING\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!python element_dist.py -r emr --emr-job-flow-id  $job_flow_id /home/ubuntu/UCSD_BigData/data/weather/ALL.head.csv > counts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile temp_dist.py\n",
      "## summarize all TMAX and TMIN measurements across stations\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "summarize the distribution of tmax and tmin\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "from sys import stderr\n",
      "\n",
      "class temp(MRJob):\n",
      "\n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            elements=line.split(',')\n",
      "            if elements[1]=='TMAX':\n",
      "                out=('tmax',1)\n",
      "            elif elements[1]=='TMIN':\n",
      "                out=('tmin',1)\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+line)\n",
      "            stderr.write(e)\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "            out=('error',1)\n",
      "\n",
      "        finally:\n",
      "            yield out\n",
      "            \n",
      "    def combiner(self, word, counts):\n",
      "        self.increment_counter('MrJob Counters','combiner',1)\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "    def reducer(self, word, counts):\n",
      "        self.increment_counter('MrJob Counters','reducer',1)\n",
      "        yield (word, sum(counts))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    temp.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting temp_dist.py\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!python element_dist.py -r local /home/ubuntu/UCSD_BigData/data/weather/ALL.head.csv > temp_dist.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## copy Weather.GHNC from professor's bucket\n",
      "from boto.s3.connection import S3Connection,OrdinaryCallingFormat\n",
      "from boto.s3.key import Key\n",
      "conn = S3Connection(key_id, secret_key,calling_format = boto.s3.connection.OrdinaryCallingFormat())\n",
      "weather = conn.get_bucket(\"Weather.GHNC\")\n",
      "# download a S3 file to local disk\n",
      "# key = weather.get_key('Partition_Tree.pkl')\n",
      "# key.get_contents_to_filename('/home/ubuntu/UCSD_BigData/data/weather/tree.pkl')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pickle\n",
      "tree = pickle.load(open('/home/ubuntu/UCSD_BigData/data/weather/tree.pkl'))\n",
      "ptree = tree['Partition_Tree']\n",
      "pstn = tree['Partitioned_Stations']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pstn.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>latitude</th>\n",
        "      <th>longitude</th>\n",
        "      <th>elevation</th>\n",
        "      <th>state</th>\n",
        "      <th>name</th>\n",
        "      <th>GSNFLAG</th>\n",
        "      <th>HCNFLAG</th>\n",
        "      <th>WMOID</th>\n",
        "      <th>weight</th>\n",
        "      <th>Node</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>USC00515000</th>\n",
        "      <td> 20.7583</td>\n",
        "      <td>-156.3211</td>\n",
        "      <td>  944.9</td>\n",
        "      <td>   I</td>\n",
        "      <td> KULA BRANCH STN 324.5</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 91198</td>\n",
        "      <td> 23830</td>\n",
        "      <td> 000000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>KRC00914101</th>\n",
        "      <td> -2.7667</td>\n",
        "      <td>-171.7167</td>\n",
        "      <td>    3.0</td>\n",
        "      <td> NaN</td>\n",
        "      <td>         CANTON ISLAND</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td> 13113</td>\n",
        "      <td> 000000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>USC00514725</th>\n",
        "      <td> 20.7167</td>\n",
        "      <td>-156.2667</td>\n",
        "      <td> 3055.9</td>\n",
        "      <td>   I</td>\n",
        "      <td>             KOLE KOLE</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   728</td>\n",
        "      <td> 000000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>AQC00914822</th>\n",
        "      <td>-11.0500</td>\n",
        "      <td>-171.0833</td>\n",
        "      <td>    3.0</td>\n",
        "      <td>   S</td>\n",
        "      <td>          SWAIN ISLAND</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   612</td>\n",
        "      <td> 000000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>USC00511008</th>\n",
        "      <td> 20.7167</td>\n",
        "      <td>-156.2667</td>\n",
        "      <td> 3037.0</td>\n",
        "      <td>   I</td>\n",
        "      <td>  HALEAKALA SUMMIT 338</td>\n",
        "      <td> NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>  5398</td>\n",
        "      <td> 000000000</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 10 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 76,
       "text": [
        "             latitude  longitude  elevation state                   name  \\\n",
        "USC00515000   20.7583  -156.3211      944.9     I  KULA BRANCH STN 324.5   \n",
        "KRC00914101   -2.7667  -171.7167        3.0   NaN          CANTON ISLAND   \n",
        "USC00514725   20.7167  -156.2667     3055.9     I              KOLE KOLE   \n",
        "AQC00914822  -11.0500  -171.0833        3.0     S           SWAIN ISLAND   \n",
        "USC00511008   20.7167  -156.2667     3037.0     I   HALEAKALA SUMMIT 338   \n",
        "\n",
        "            GSNFLAG HCNFLAG  WMOID  weight       Node  \n",
        "USC00515000     NaN     NaN  91198   23830  000000000  \n",
        "KRC00914101     NaN     NaN    NaN   13113  000000000  \n",
        "USC00514725     NaN     NaN    NaN     728  000000000  \n",
        "AQC00914822     NaN     NaN    NaN     612  000000000  \n",
        "USC00511008     NaN     NaN    NaN    5398  000000000  \n",
        "\n",
        "[5 rows x 10 columns]"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile sample.py\n",
      "# sample hdfs:/weather/weather.csv to make a test set.\n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "sample ration 1/1000\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "from sys import stderr\n",
      "import random\n",
      "\n",
      "\n",
      "class sample(MRJob):\n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            if random.random()<0.1:\n",
      "                yield (None,line)\n",
      "        except Exception, e:\n",
      "            stderr.write(str(e))\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    sample.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting sample.py\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# clean the data and create a subset\n",
      "subdir=\"/home/ubuntu/UCSD_BigData/data/weather/\"\n",
      "dat = file(subdir+\"subset\").readlines()\n",
      "fout=file(subdir+\"subset_9k\",\"wb\")\n",
      "for d in dat:\n",
      "    fout.write(d.split(\"\\t\")[1].split(\"\\n\")[0].strip('\\\"')+\"\\n\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subset9k= subdir+\"subset_9k\"\n",
      "#!less $subset9k"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 173
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile meas_count.py\n",
      "## count number of valid station/year. >50% TMAX and TMIN \n",
      "#!/usr/bin/python\n",
      "\"\"\"\n",
      "count the number of valid measurements. A valide record has at least 50% days with TMAX and TMIN.\n",
      "\"\"\"\n",
      "import sys\n",
      "sys.path.append('/usr/lib/python2.6/dist-packages')\n",
      "from mrjob.job import MRJob\n",
      "import re\n",
      "from sys import stderr\n",
      "\n",
      "class countMeas(MRJob):\n",
      "    def mapper(self, _, line):\n",
      "        try:\n",
      "            self.increment_counter('MrJob Counters','mapper-all',1)\n",
      "            rec = line.split(\",\")\n",
      "            stn,meas,year = rec[:3]\n",
      "            stderr.write(stn+\"\\t\"+meas+\"\\t\"+year)\n",
      "            if meas=='TMAX' or meas=='TMIN': # this is measurement is about tmax of tmin\n",
      "                if len(filter(None,rec[3:]))/365.0>0.5: # measurements during >50% days of the year \n",
      "                    stderr.write(\"mapper: \"+\",\".join(rec[:3]))\n",
      "                    yield (stn,1)\n",
      "        except Exception, e:\n",
      "            stderr.write('Error in line:\\n'+str(line[:5]))\n",
      "            stderr.write(str(e))\n",
      "            self.increment_counter('MrJob Counters','mapper-error',1)\n",
      "\n",
      "    def reducer(self, key, value):\n",
      "        self.increment_counter('MrJob Counters','reducer',1)\n",
      "        try:\n",
      "            l_counts=[v for v in value]\n",
      "            s=sum(l_counts)\n",
      "            yield (key,s)\n",
      "\n",
      "        except Exception as e:\n",
      "            stderr.write('Error:')\n",
      "            stderr.write(str(e))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    countMeas.run()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting meas_count.py\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# !python meas_count.py -r local /home/ubuntu/UCSD_BigData/data/weather/subset_9k  > meas_weight"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!python meas_count.py -r emr --emr-job-flow-id  $job_flow_id hdfs:/weather/weather.csv  > meas_weight\n",
      "#hdfs:/weather/weather.csv > /home/ubuntu/UCSD_BigData/data/weather/subset  "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import re\n",
      "import pandas as pd\n",
      "import pickle\n",
      "# create or load a dataframe of station meta data\n",
      "try:\n",
      "    stationDF=pickle.load(file(os.path.join(dat_dir,\"stationDF.pkl\")))\n",
      "except:\n",
      "    stnDat = file(os.path.join(dat_dir,\"ghcnd-stations.txt\"),'rb').readlines()\n",
      "    stn = [re.split(\" +\", st)[:4] for st in stnDat]\n",
      "    ind = [l[0] for l in stn] # get the index\n",
      "    stn = [[st[0],float(st[1]),float(st[2]),float(st[3])] for st in stn]\n",
      "    stnDF = pd.DataFrame(data=stn,index=ind,columns=['ID','latitude','longitude','elevation'])\n",
      "    \n",
      "#     pickle.dump(stnDF,file(os.path.join(dat_dir,\"stationDF.pkl\"),\"wb\"))\n",
      "    # append a column of measurement weights to this metadata dataframe\n",
      "    measWT = file(\"meas_weight\").readlines()\n",
      "    measWT=[filter(None,re.split(\"[\\t\\n\\\"]\",rec)) for rec in measWT]\n",
      "    measWT=[[x[0],int(x[1])] for x in measWT]\n",
      "    ind = [m[0] for m in measWT]\n",
      "    measDF = pd.DataFrame(data=measWT,index=ind,columns=[\"ID\",\"weight\"])\n",
      "    del measDF[\"ID\"]\n",
      "    stationDF = stnDF.merge(measDF,how='outer',left_index=True,right_index=True)\n",
      "    pickle.dump(stationDF,file(os.path.join(dat_dir,\"stationDF.pkl\"),\"wb\"))\n",
      "    \n",
      "finally:\n",
      "    print stationDF[stationDF['latitude']>30].head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                      ID  latitude  longitude  elevation  weight\n",
        "AF000040930  AF000040930   35.3170    69.0170       3366     NaN\n",
        "AG000060390  AG000060390   36.7167     3.2500         24     NaN\n",
        "AG000060590  AG000060590   30.5667     2.8667        397       1\n",
        "AGE00135039  AGE00135039   35.7297     0.6500         50     NaN\n",
        "AJ000037575  AJ000037575   41.5500    46.6670        490     NaN\n",
        "\n",
        "[5 rows x 5 columns]\n"
       ]
      }
     ],
     "prompt_number": 239
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 228
    }
   ],
   "metadata": {}
  }
 ]
}